import pandas as pd 
import numpy as np  
from tabulate import tabulate 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn import metrics 
import nltk 
import string 
import re 
from nltk.stem import WordNetLemmatizer 
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import accuracy_score, recall_score 
from sklearn.preprocessing import MinMaxScaler 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.svm import SVC 
from sklearn.svm import LinearSVC 
from sklearn import metrics 
nltk.download('vader_lexicon') 
import warnings 
warnings.filterwarnings('ignore') 
# --- Step Separator --- 
Suicide = pd.read_csv('Suicide_Detection.csv') 
data_split = np.array_split(Suicide, 3) 
Suicide = data_split[0] 
Suicide = Suicide.drop('Unnamed: 0',axis=1) 
# --- Step Separator --- 
nltk.download('stopwords') 
stopwords = nltk.corpus.stopwords.words('english') 
lemmatizer = WordNetLemmatizer() 
# --- Step Separator --- 
X = Suicide.drop('class', axis=1) 
26 
 
27 
 
y = Suicide['class'] 
 
# --- Step Separator --- 
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) 
 
# --- Step Separator --- 
 
tfidf_vectorizer = TfidfVectorizer(max_features=10000) 
tfidf_vectorizer_n12=TfidfVectorizer(max_features=10000, 
ngram_range=(1,2)) 
 
X_tfidf_train = tfidf_vectorizer.fit_transform(X_train['text']) 
X_tfidf_test = tfidf_vectorizer.transform(X_test['text']) 
 
X_tfidf_train_n12= tfidf_vectorizer_n12.fit_transform(X_train['text']) 
X_tfidf_test_n12=tfidf_vectorizer_n12.transform(X_test['text']) 
 
# --- Step Separator --- 
 
vectorizer = CountVectorizer()  
X_bow_train = vectorizer.fit_transform(X_train['text']) 
X_bow_test = vectorizer.transform(X_test['text']) 
 
# --- Step Separator --- 
 
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA 
def get_vader_scores(data): 
    sid=SIA() 
    vader_df=data.copy() 
   vader_df['scores'] = vader_df['text'].apply(lambda txt: 
sid.polarity_scores(str(txt))) 
     
    vader_df['neg_score'] = vader_df['scores'].apply(lambda txt: txt['neg']) 
    vader_df['neu_score'] =vader_df['scores'].apply(lambda txt: txt['neu']) 
    vader_df['pos_score'] = vader_df['scores'].apply(lambda txt: txt['pos']) 
 vader_df['compound']= vader_df['scores'].apply(lambda txt:txt['compound']) 
    vader_df.drop('scores', axis=1, inplace=True) 
    vader_df.drop('text', axis=1, inplace=True) 
    return vader_df 
 
# --- Step Separator --- 
 
 
28 
 
X_vader_train = get_vader_scores(X_train) 
X_vader_test= get_vader_scores(X_test) 
 
# --- Step Separator --- 
 
from sklearn.svm import LinearSVC 
from sklearn.feature_selection import SelectFromModel 
 
# We Can select any model but linearSVC has l1 norm penality which deals 
with sparse 
lsvc = LinearSVC(C=100, penalty='l1', max_iter=500, dual=False) 
lsvc.fit(X_tfidf_train, y_train) 
 
# This function select the best features that has high weigh 
fs = SelectFromModel(lsvc, prefit=True) 
# This function redeuce X to the selected features 
X_selection = fs.transform(X_tfidf_train) 
X_test_selection = fs.transform(X_tfidf_test) 
 
lsvc.fit(X_tfidf_train_n12, y_train) 
fs_n12 = SelectFromModel(lsvc, prefit=True) 
X_selection_n12 = fs_n12.transform(X_tfidf_train_n12) 
X_test_selection_n12 = fs_n12.transform(X_tfidf_test_n12) 
 
lsvc.fit(X_bow_train, y_train) 
fs_n12 = SelectFromModel(lsvc, prefit=True) 
X_selection_bow = fs_n12.transform(X_bow_train) 
X_test_selection_bow = fs_n12.transform(X_bow_test) 
 
# --- Step Separator --- 
 
import matplotlib.pyplot as plt 
def plot_results(data):   
    barWidth = 0.15 
    # set heights of bars 
    bars1 = [data[0][1],data[1][1], data[2][1]] 
    bars2 = [data[0][2], data[1][2], data[2][2]] 
    bars3 = [data[0][3], data[1][3], data[2][3]] 
    bars4 = [data[0][4], data[1][4], data[2][4]] 
     
    # Set position of bar on X axis 
    r1 = np.arange(len(bars1)) 
    r2 = [x + barWidth for x in r1] 
 
29 
 
    r3 = [x + barWidth for x in r2] 
    r4 = [x + barWidth for x in r3] 
     
    # Make the plot 
    plt.bar(r1, bars1, color='r', width=barWidth, edgecolor='white', label='tfidf') 
    plt.bar(r2, bars2, color='b', width=barWidth, edgecolor='white', 
label='tfidf_n12') 
    plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', 
label='bow') 
    plt.bar(r4, bars4, color='#9a7f5e', width=barWidth, edgecolor='white', 
label='vader')   
 
    # Add xticks on the middle of the group bars 
    plt.xlabel('group', fontweight='bold') 
    plt.xticks([r + barWidth for r in range(len(bars1))], ['precision', 'recall', 'f1
score']) 
    # Create legend & Show graphic 
    plt.legend() 
    plt.grid() 
    plt.show() 
 
# --- Step Separator --- 
 
lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False) 
lsvc.fit(X_selection, y_train) 
y_predict_tfidf = lsvc.predict(X_test_selection) 
 
lsvc.fit(X_selection_n12,y_train) 
y_predict_tfidf_n12 = lsvc.predict(X_test_selection_n12) 
 
lsvc.fit(X_selection_bow,y_train) 
y_predict_bow = lsvc.predict(X_test_selection_bow) 
 
lsvc.fit(X_vader_train,y_train) 
y_predict_vader = lsvc.predict(X_vader_test) 
 
linear_svm_tfidf_results=metrics.precision_recall_fscore_support(y_test, 
y_predict_tfidf) 
linear_svm_tfidf_n12_results=metrics.precision_recall_fscore_support(y_tes
t, y_predict_tfidf_n12) 
linear_svm_bow_results=metrics.precision_recall_fscore_support(y_test, 
y_predict_bow) 
 
30 
 
vader_svm_results=metrics.precision_recall_fscore_support(y_test, 
y_predict_vader) 
 
# --- Step Separator --- 
 
tfidf_acc= metrics.accuracy_score(y_test, y_predict_tfidf) 
tfidf_n12_acc=accuracy_score(y_test, y_predict_tfidf_n12) 
bow_acc= accuracy_score(y_test, y_predict_bow) 
vader_acc=accuracy_score(y_test, y_predict_vader) 
 
# --- Step Separator --- 
 
data1 = [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'], 
         ['precision',linear_svm_tfidf_results[0][0],linear_svm_tfidf_n12_result
s[0][0],linear_svm_bow_results[0][0],vader_svm_results[0][0]], 
         ['recall',linear_svm_tfidf_results[1][0],linear_svm_tfidf_n12_results[1]
[0],linear_svm_bow_results[1][0], 
          vader_svm_results[1][0]], 
         ['F1
score',linear_svm_tfidf_results[2][0],linear_svm_tfidf_n12_results[2][0],line
ar_svm_bow_results[2][0], vader_svm_results[2][0]], 
        ['accuracy',tfidf_acc,tfidf_n12_acc,bow_acc,vader_acc]] 
 
# --- Step Separator --- 
 
print(tabulate(data1,headers='firstrow',tablefmt='fancy_grid')) 
 
# --- Step Separator --- 
 
plot_results(data1[1:]) 
 
# --- Step Separator --- 
 
clf = RandomForestClassifier(max_depth=10) 
clf.fit(X_selection, y_train) 
y_predict_tfidf_2 = clf.predict(X_test_selection) 
clf.fit(X_selection_n12, y_train) 
y_predict_tfidf_n12_2 = clf.predict(X_test_selection_n12) 
 
clf.fit(X_selection_bow, y_train) 
y_predict_bow_2 = clf.predict(X_test_selection_bow) 
 
clf.fit(X_vader_train, y_train) 
 
31 
 
y_predict_vader_2 = clf.predict(X_vader_test) 
 
# --- Step Separator --- 
 
RandomForest_tfidf_results=metrics.precision_recall_fscore_support(y_test, 
y_predict_tfidf_2) 
RandomForest_tfidf_n12_results=metrics.precision_recall_fscore_support(y
_test, y_predict_tfidf_n12_2) 
RandomForest_bow_results=metrics.precision_recall_fscore_support(y_test, 
y_predict_bow_2) 
RandomForest_vader_results=metrics.precision_recall_fscore_support(y_tes
t, y_predict_vader_2) 
 
# --- Step Separator --- 
 
RandomForest_tfidf_acc= metrics.accuracy_score(y_test, y_predict_tfidf_2) 
RandomForest_tfidf_n12_acc=accuracy_score(y_test, 
y_predict_tfidf_n12_2) 
RandomForest_bow_acc= accuracy_score(y_test, y_predict_bow_2) 
RandomForest_vader_acc=accuracy_score(y_test, y_predict_vader_2) 
 
# --- Step Separator --- 
 
data2 = [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'], 
         ['precision',RandomForest_tfidf_results[0][0],RandomForest_tfidf_n12
_results[0][0],RandomForest_bow_results[0][0], 
RandomForest_vader_results[0][0]], 
         ['recall',RandomForest_tfidf_results[1][0],RandomForest_tfidf_n12_re
sults[1][0],RandomForest_bow_results[1][0], 
          RandomForest_vader_results[1][0]], 
   ['F1
score',RandomForest_tfidf_results[2][0],RandomForest_tfidf_n12_results[2][0
],RandomForest_bow_results[2][0], 
          RandomForest_vader_results[2][0]], 
        ['accuracy',RandomForest_tfidf_acc,RandomForest_tfidf_n12_acc, 
RandomForest_bow_acc,RandomForest_vader_acc]] 
 
# --- Step Separator --- 
 
print(tabulate(data2,headers='firstrow',tablefmt='fancy_grid')) 
 
# --- Step Separator --- 
 
plot_results(data2[1:]) 
# --- Step Separator --- 
from sklearn.ensemble import BaggingClassifier 
from sklearn.tree import DecisionTreeClassifier 
bag_clf = BaggingClassifier( 
DecisionTreeClassifier(), n_estimators=500, 
max_samples=100, bootstrap=True, n_jobs=-1) 
bag_clf.fit(X_selection, y_train) 
y_pred_5 = bag_clf.predict(X_test_selection) 
bag_clf.fit(X_selection_n12, y_train) 
y_pred_n12_5 = bag_clf.predict(X_test_selection_n12) 
bag_clf.fit(X_selection_bow, y_train) 
y_pred_bow_5 = bag_clf.predict(X_test_selection_bow) 
bag_clf.fit(X_vader_train, y_train) 
y_pred_vader_5 = bag_clf.predict(X_vader_test) 
# --- Step Separator --- 
bag_tfidf_results=metrics.precision_recall_fscore_support(y_test, y_pred_5) 
bag_tfidf_n12_results=metrics.precision_recall_fscore_support(y_test, 
y_pred_n12_5) 
bag_bow_results=metrics.precision_recall_fscore_support(y_test, 
y_pred_bow_5) 
bag_vader_results=metrics.precision_recall_fscore_support(y_test, 
y_pred_vader_5) 
# --- Step Separator --- 
bag_tfidf_acc= metrics.accuracy_score(y_test, y_pred_5) 
bag_tfidf_n12_acc=accuracy_score(y_test, y_pred_n12_5) 
bag_bow_acc= accuracy_score(y_test, y_pred_bow_5) 
bag_vader_acc=accuracy_score(y_test, y_pred_vader_5) 
# --- Step Separator --- 
data4= [['TF-IDF','TF-IDF 2-grams ','bag of words','vader'], 
32 
 
33 
 
        ['precision',bag_tfidf_results[0][0],bag_tfidf_n12_results[0][0],bag_bo
w_results[0][0], bag_vader_results[0][0]], 
         ['recall',bag_tfidf_results[1][0],bag_tfidf_n12_results[1][0],bag_bow_r
esults[1][0],bag_vader_results[1][0]], 
         ['F1
score',bag_tfidf_results[2][0],bag_tfidf_n12_results[2][0],bag_bow_results[2
][0],bag_vader_results[2][0]], 
        ['accuracy',bag_tfidf_acc,bag_tfidf_n12_acc, bag_bow_acc, 
          bag_vader_acc]] 
 
# --- Step Separator --- 
 
print(tabulate(data4,headers='firstrow',tablefmt='fancy_grid')) 
 
# --- Step Separator --- 
 
plot_results(data4[1:]) 
 
# --- Step Separator --- 
 
import numpy as np 
import pandas as pd 
import seaborn as sns 
import matplotlib 
import matplotlib.pyplot as plt 
%matplotlib inline 
 
# --- Step Separator --- 
 
import demoji 
import re 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn. model_selection import train_test_split 
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer 
from nltk.probability import FreqDist 
import nltk 
 
import unicodedata 
 
from wordcloud import WordCloud 
 
nltk.download('stopwords') 
port_stem=PorterStemmer() 
# --- Step Separator --- 
suicide_df = pd.read_csv('Suicide_Detection.csv') 
# --- Step Separator --- 
suicide_df.shape 
# --- Step Separator --- 
suicide_df['class'].unique() 
# --- Step Separator --- 
suicide_df.isnull().sum() 
# --- Step Separator --- 
suicide_df.duplicated().sum() 
# --- Step Separator --- 
suicide_df 
# --- Step Separator --- 
suicide_df['class'].value_counts() 
# --- Step Separator --- 
#Creating a Sample Class from both the given classes 
suicide_samples     = suicide_df[suicide_df['class'] == 'suicide'].sample(4000, 
random_state=10) 
non_suicide_samples 
= 
suicide_df[suicide_df['class'] 
suicide'].sample(4000, random_state=10) 
# --- Step Separator --- 
== 
suicide_df_2 = pd.concat([suicide_samples, non_suicide_samples]) 
suicide_df_2 
'non
34 
 
35 
 
 
# --- Step Separator --- 
 
def clean_text(text): 
    text = re.sub(r"\S*https?:\S*", '', text, flags=re.MULTILINE)  # Remove 
links in text 
    text = demoji.replace(text, '') 
         
#  text = unicodedata.normalize('NFKD', text).encode('ascii', 
'ignore').decode('ascii') 
    text = re.sub('[^a-zA-Z]', ' ', text) 
    text = text.lower() 
    text = text.strip() 
    text = text.split() 
    text = [port_stem.stem(word) for word in text if not word in 
stopwords.words('english')] 
    text = ' '.join(text) 
    return text 
 
# --- Step Separator --- 
 
X = suicide_df_2['text'] 
y = suicide_df_2['class'] 
 
# --- Step Separator --- 
 
y = y.map({'suicide': 1, 'non-suicide': 0}) 
 
# --- Step Separator --- 
 
vector = TfidfVectorizer() 
X      = vector.fit_transform(X) 
 
# --- Step Separator --- 
 
print(X) 
 
# --- Step Separator --- 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, 
stratify= y,shuffle = True, random_state = 77) 
 
# --- Step Separator --- 
 
print('Train size:', X_train.shape) 
print('Test  size:', X_test.shape) 
# --- Step Separator --- 
# Models 
from sklearn.linear_model import LogisticRegression 
from sklearn.svm import SVC 
from sklearn.naive_bayes import GaussianNB 
# Metrics 
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, 
f1_score, precision_score 
from sklearn import metrics 
# --- Step Separator --- 
def evaluate_model(y_pred, y_test): 
print('Accuracy:', accuracy_score(y_pred,y_test)) 
print('Recall:', recall_score(y_pred,y_test)) 
print('F1 Score', f1_score(y_pred,y_test)) 
print('Precision', precision_score(y_pred,y_test)) 
print('Confusion Matrix:\n' ) 
sns.heatmap(confusion_matrix(y_pred,y_test), annot=True, fmt='g'); 
# --- Step Separator --- 
model = LogisticRegression(random_state=7) 
model.fit(X_train, y_train) 
y_pred = model.predict(X_test) 
# --- Step Separator --- 
evaluate_model(y_pred,y_test) 
# --- Step Separator --- 
frase = "I would like one more chance, but I can't take it anymore" 
frase = clean_text(frase) 
t     
= vector.transform(pd.Series(frase)) 
pred  = model.predict(t) 
pred 
36 
# --- Step Separator --- 
model = SVC(kernel='linear', random_state=77) 
model.fit(X_train, y_train) 
y_pred = model.predict(X_test) 
# --- Step Separator --- 
evaluate_model(y_pred, y_test) 
# --- Step Separator --- 
frase = "i’m tearing myself apart i need a girl to love and talk to and confide 
in i’m going insane" 
frase = clean_text(frase) 
t     
= vector.transform(pd.Series(frase)) 
pred  = model.predict(t) 
pred 
# --- Step Separator --- 
import pandas as pd 
import numpy as np 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, 
Bidirectional, GlobalMaxPool1D 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report 
# --- Step Separator --- 
# Load your data 
df = pd.read_csv('Suicide_Detection.csv')  # Replace with actual path 
# Make sure it has 'text' and 'label' columns 
df.head() 
# --- Step Separator --- 
37 
 
38 
 
# Prepare texts and labels 
texts = df['text'].astype(str).tolist() 
labels = df['class'].tolist() 
 
# Split into train and validation sets 
train_texts, val_texts, train_labels, val_labels = train_test_split( 
    texts, labels, test_size=0.1, random_state=42, stratify=labels) 
 
# Tokenize 
max_words = 20000  # Only keep top 20k words 
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>") 
tokenizer.fit_on_texts(train_texts) 
 
# Convert texts to sequences 
train_sequences = tokenizer.texts_to_sequences(train_texts) 
val_sequences = tokenizer.texts_to_sequences(val_texts) 
 
# Pad sequences 
max_len = 200  # Maximum length of a post (you can adjust) 
train_padded = pad_sequences(train_sequences, maxlen=max_len, 
padding='post') 
val_padded = pad_sequences(val_sequences, maxlen=max_len, 
padding='post') 
 
train_labels = np.array(train_labels) 
val_labels = np.array(val_labels) 
 
# --- Step Separator --- 
 
model = Sequential([Embedding(input_dim=max_words, output_dim=128, 
input_length=max_len), 
    Bidirectional(LSTM(64, return_sequences=True)), 
    GlobalMaxPool1D(), 
    Dense(64, activation='relu'), 
    Dropout(0.5), 
    Dense(1, activation='sigmoid')]) 
model.compile( 
    loss='binary_crossentropy', 
    optimizer='adam', 
    metrics=['accuracy']) 
 
model.summary() 
 
# --- Step Separator --- 
from tensorflow.keras.callbacks import EarlyStopping 
early_stop = EarlyStopping( 
monitor='val_accuracy', 
patience=3, 
restore_best_weights=True) 
# --- Step Separator --- 
print(train_labels.dtype)  # Make sure it's float32 or int (ideally) 
print(val_labels.dtype)  # Same for validation labels 
# --- Step Separator --- 
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
tokenizer = Tokenizer(num_words=10000) 
tokenizer.fit_on_texts(train_texts)  # train_texts is your raw text data 
train_sequences = tokenizer.texts_to_sequences(train_texts) 
val_sequences = tokenizer.texts_to_sequences(val_texts) 
train_padded = pad_sequences(train_sequences, padding='post', maxlen=100) 
val_padded = pad_sequences(val_sequences, padding='post', maxlen=100) 
# --- Step Separator --- 
import numpy as np 
num_classes = len(np.unique(train_labels))  # Assuming train_labels contains 
the label data 
# --- Step Separator --- 
from sklearn.preprocessing import LabelEncoder 
label_encoder = LabelEncoder() 
train_labels = label_encoder.fit_transform(train_labels) 
val_labels = label_encoder.transform(val_labels) 
num_classes = len(np.unique(train_labels))  # Now you can define 
num_classes 
39 
# --- Step Separator --- 
from tensorflow.keras.utils import to_categorical 
train_labels = to_categorical(train_labels, num_classes=num_classes) 
val_labels = to_categorical(val_labels, num_classes=num_classes) 
# --- Step Separator --- 
train_labels = np.argmax(train_labels, axis=1) 
val_labels = np.argmax(val_labels, axis=1) 
# --- Step Separator --- 
train_labels = train_labels.reshape(-1)  # Flatten the 2D array to 1D 
val_labels = val_labels.reshape(-1)      
# Flatten the 2D array to 1D 
# --- Step Separator --- 
print(train_labels.shape)  # Check the shape of the train_labels 
print(train_labels[:5])    # Check the first few values of the labels 
# --- Step Separator --- 
model.add(Embedding(input_dim=10000,output_dim=128, 
input_length=100)) 
# --- Step Separator --- 
model.add(Dense(1, activation='sigmoid'))  # 1 output unit with sigmoid 
activationmodel.add(Dense(1, activation='sigmoid'))  # 1 output unit with 
sigmoid activation 
# --- Step Separator --- 
val_labels = val_labels.reshape(-1, 1) 
# --- Step Separator --- 
# Assuming binary classification, output layer with a single unit 
model.add(Dense(1, activation='sigmoid'))  # Single output unit with sigmoid 
activation 
40 
# Ensure labels are reshaped if necessary 
val_labels = val_labels.reshape(-1, 1) 
# Compile model with binary_crossentropy loss 
model.compile(optimizer='adam',loss='binary_crossentropy', 
metrics=['accuracy']) 
# Evaluate the model 
val_loss, val_accuracy = model.evaluate(val_padded, val_labels) 
print(f"Validation Accuracy: {val_accuracy:.4f}") 
# --- Step Separator --- 
val_preds = (model.predict(val_padded) > 0.5).astype("int32") 
# --- Step Separator --- 
print(f"val_labels shape: {val_labels.shape}") 
print(f"val_preds shape: {val_preds.shape}") 
# --- Step Separator --- 
print(f"Unique values in val_labels: {np.unique(val_labels)}") 
print(f"Unique values in val_preds: {np.unique(val_preds)}") 
# --- Step Separator --- 
val_labels = (val_labels > 0).astype("int32") 
# Ensure val_preds are binary 
val_preds = (model.predict(val_padded) > 0.5).astype("int32") 
# --- Step Separator --- 
val_preds = val_preds.ravel()  # This converts it to a 1D array 
# --- Step Separator --- 
from sklearn.metrics import accuracy_score 
accuracy = accuracy_score(val_labels, val_preds) 
print(f"Accuracy: {accuracy:.4f}")
